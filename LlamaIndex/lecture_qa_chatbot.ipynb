{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78c5456",
   "metadata": {},
   "source": [
    "# Lecture Q&A Chatbot with Context Caching for Efficient Retrieval\n",
    "## Background and Purpose\n",
    "This workflow implements an advanced Q&A chatbot using LlamaIndex and the Gemini API, designed for students to analyze lecture materials by uploading a class presentation (PDF) and an audio recording. It leverages context caching to store both files for efficient retrieval, reducing API calls for repeated queries, and supports multi-turn conversations for iterative exploration of lecture content. The chatbot is ideal for academic use, enabling students to extract key points and clarify concepts from complex lecture materials with high accuracy and minimal latency.\n",
    "\n",
    "## Use Case\n",
    "A student attending a machine learning course uploads the lecture slides (PDF) and an audio recording of the class to the chatbot. They ask, \"Summarize the key points from the lecture,\" to get a concise overview of the material. Later, they follow up with, \"Explain the main concept discussed in slide 5,\" to dive deeper into a specific topic, such as neural network architectures. The chatbot uses cached content to efficiently process both queries, providing accurate, context-aware answers without repeatedly uploading files.\n",
    "\n",
    "## Import Required Libraries\n",
    "This cell imports necessary libraries, including Google’s generative AI client for file uploads and caching, LlamaIndex’s `GoogleGenAI` for LLM interactions, and components for handling chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68730672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import CreateCachedContentConfig, Content, Part\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99b555",
   "metadata": {},
   "source": [
    "## Upload and Cache Lecture Files\n",
    "Defines a function to upload a PDF presentation and an audio recording to the Gemini API, wait for processing, and create a cache with a one-hour TTL containing both files. The cache enables efficient retrieval of lecture content, with a system instruction ensuring accurate and detailed answers based on the provided materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd5a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_cache_files(pdf_path: str, audio_path: str, api_key: str) -> str:\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # Upload PDF\n",
    "    pdf_file = client.files.upload(file=pdf_path)\n",
    "    while pdf_file.state.name == \"PROCESSING\":\n",
    "        time.sleep(2)\n",
    "        pdf_file = client.files.get(name=pdf_file.name)\n",
    "    \n",
    "    # Upload Audio\n",
    "    audio_file = client.files.upload(file=audio_path)\n",
    "    while audio_file.state.name == \"PROCESSING\":\n",
    "        time.sleep(2)\n",
    "        audio_file = client.files.get(name=audio_file.name)\n",
    "    \n",
    "    # Create cache with both files\n",
    "    cache = client.caches.create(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        config=CreateCachedContentConfig(\n",
    "            display_name=\"Lecture Q&A Cache\",\n",
    "            system_instruction=(\n",
    "                \"You are an expert lecture assistant. Provide accurate and detailed \"\n",
    "                \"answers based on the provided lecture presentation (PDF) and audio recording.\"\n",
    "            ),\n",
    "                contents=[\n",
    "                Content(\n",
    "                    role=\"user\",\n",
    "                    parts=[\n",
    "                        Part.from_uri(file_uri=pdf_file.uri, mime_type=\"application/pdf\"),\n",
    "                        Part.from_uri(file_uri=audio_file.uri, mime_type=\"audio/mp3\"),\n",
    "                    ],\n",
    "                )\n",
    "            ],\n",
    "            ttl=\"3600s\",\n",
    "        ),\n",
    "    )\n",
    "    print(\"Cache created successfully\")\n",
    "    return cache.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709565ed",
   "metadata": {},
   "source": [
    "## Create Q&A Messages\n",
    "Constructs a list of `ChatMessage` objects for the conversation, appending a new user query to any existing messages. This supports multi-turn conversations by maintaining context for follow-up questions without directly embedding files in each query, as they are stored in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29513c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_messages(query: str, previous_messages: list = None) -> list:\n",
    "    messages = previous_messages or []\n",
    "    messages.append(ChatMessage(role=\"user\", content=query))\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ef790",
   "metadata": {},
   "source": [
    "## Execute Workflow\n",
    "Runs the chatbot with a sample initial query (\"Summarize the key points from the lecture presentation and audio.\"), a follow-up query (\"Explain the main concept discussed in slide 5 of the presentation.\"), and paths to a PDF and audio file. Replace `sample_lecture_slides.pdf` and `sample_lecture_audio.mp3` with real file paths (e.g., a lecture presentation and recording) to test functionality. Prints the results from both turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading and caching files...\n",
      "Cache created successfully\n",
      "\n",
      "Initial Response:\n",
      "assistant: This lecture provides a detailed introduction to Convolutional Neural Networks (CNNs), covering their historical development, core architectural components, and practical applications.\n",
      "\n",
      "Here are the key points:\n",
      "\n",
      "**1. Recap of Neural Networks (NNs):**\n",
      "*   Last time, we discussed basic Neural Networks, which stack linear layers with non-linearities (activation functions).\n",
      "*   NNs can address problems like the \"mode problem\" by learning intermediate templates (e.g., different types of cars) and combining them for a final classification score.\n",
      "\n",
      "**2. A Bit of History:**\n",
      "*   **Perceptron (1957, Frank Rosenblatt):** The Mark I Perceptron was the first implementation of the perceptron algorithm. It used 20x20 photocells (400 pixels) to recognize letters. It had an update rule similar to backpropagation but lacked a principled backpropagation technique.\n",
      "*   **Adaline/Madaline (1960, Widrow and Hoff):** Developed multi-layer perceptron networks, but still without a principled training method like backprop.\n",
      "*   **Backpropagation (1986, Rumelhart et al.):** This was the first time backpropagation became popular, allowing for principled training of multi-layer networks using the chain rule.\n",
      "*   **Deep Learning Revival (2006, Hinton and Salakhutdinov):** Showed that deep neural networks could be trained effectively, often using careful initialization techniques like pre-training with Restricted Boltzmann Machines (RBMs) followed by fine-tuning.\n",
      "*   **Breakthrough Results (2010-2012):**\n",
      "    *   **Speech Recognition:** Strong results using Deep Belief Networks (Mohamed, Dahl, Hinton, 2010) and pre-trained Deep Neural Networks (Dahl, Yu, Deng, Acero, 2012).\n",
      "    *   **ImageNet Classification (AlexNet, 2012, Krizhevsky, Sutskever, Hinton):** This was a landmark paper that dramatically reduced error rates on the ImageNet benchmark, leveraging larger, deeper networks, massive datasets, and GPUs.\n",
      "\n",
      "**3. Biological Inspiration for CNNs:**\n",
      "*   **Hubel & Wiesel (1959-1968):** Their work on the cat's visual cortex revealed:\n",
      "    *   **Topographical Mapping:** Nearby cells in the cortex represent nearby regions in the visual field.\n",
      "    *   **Hierarchical Organization:** Neurons respond to increasingly complex features:\n",
      "        *   Retinal ganglion cells: Respond to circular spots.\n",
      "        *   Simple cells: Respond to oriented edges.\n",
      "        *   Complex cells: Respond to oriented edges and movement.\n",
      "        *   Hypercomplex cells: Respond to movement with an endpoint (corners, blobs).\n",
      "*   **Neocognitron (1980, Fukushima):** The first network architecture inspired by this hierarchy, using alternating \"simple cells\" (modifiable parameters) and \"complex cells\" (performing pooling for invariance).\n",
      "*   **Gradient-based Learning for Document Recognition (1998, LeCun et al. - LeNet-5):** Applied CNNs to recognize digits in zip codes, showing practical success.\n",
      "\n",
      "**4. Modern Applications of CNNs (\"ConvNets are Everywhere\"):**\n",
      "*   **Image Classification & Retrieval:** Highly accurate.\n",
      "*   **Object Detection:** Localizing objects with bounding boxes (e.g., Faster R-CNN).\n",
      "*   **Image Segmentation:** Labeling every pixel of an object (e.g., trees, people).\n",
      "*   **Self-Driving Cars:** Core technology for perception.\n",
      "*   **Face Recognition, Video Analysis, Game Playing (e.g., Atari, Go), Medical Imaging, Galaxy Classification, Street Sign Recognition, Whale Recognition, Aerial Maps.**\n",
      "*   **Image Captioning:** Generating natural language descriptions of images.\n",
      "*   **Artistic Applications:** Deep Dream (hallucinating objects), Neural Style Transfer (re-rendering images in an artistic style).\n",
      "*   These applications are powered by powerful GPUs, enabling efficient training and inference.\n",
      "\n",
      "**5. Convolutional Layer (The Core of CNNs):**\n",
      "*   **Preserves Spatial Structure:** Unlike fully connected layers that flatten inputs into vectors, CNNs maintain the 3D structure (width x height x depth) of images.\n",
      "*   **Filters (Kernels):** Small 3D arrays of weights (e.g., 5x5x3).\n",
      "    *   Filters *always* extend the full depth of the input volume.\n",
      "    *   Each filter learns to detect a specific feature (e.g., a vertical edge, a specific color blob).\n",
      "*   **Convolution Operation:** The filter \"slides\" spatially over the input image. At each spatial location, a dot product is computed between the filter's weights and the corresponding chunk of the input image. This produces a single number.\n",
      "*   **Activation Map:** Sliding one filter over the entire input image generates a 2D \"activation map\" (or \"feature map\"), where each value indicates how strongly that filter's feature is present at that spatial location.\n",
      "*   **Multiple Filters:** A convolutional layer typically uses multiple filters (e.g., 6, 10, 32, 64, etc.). Each filter produces its own activation map. These maps are stacked together to form a new 3D output volume (width x height x number of filters).\n",
      "*   **Parameters:** Each filter has `(filter_width * filter_height * input_depth) + 1` (for bias) parameters. All neurons in a single activation map *share* these parameters.\n",
      "*   **Spatial Dimension Formula:** The output spatial dimension (W2 or H2) is calculated as `(W1 - F + 2P) / S + 1`, where W1 is input size, F is filter size, P is padding, and S is stride.\n",
      "*   **Zero-Padding:** Commonly used to control the output spatial dimensions, especially to maintain the input size. For a stride of 1, padding `P = (F-1)/2` will preserve the spatial dimensions.\n",
      "*   **Motivation for Padding:** Prevents rapid shrinking of spatial dimensions in deep networks, which would lead to loss of information and edge features.\n",
      "\n",
      "**6. Brain/Neuron View of Convolutional Layer:**\n",
      "*   Each neuron in a convolutional layer has **local connectivity**: it's only connected to a small, localized region of the input (its \"receptive field\").\n",
      "*   All neurons in a single activation map **share parameters**: they detect the same feature but at different spatial locations. This makes CNNs highly parameter-efficient compared to fully connected layers.\n",
      "\n",
      "**7. Pooling Layer:**\n",
      "*   **Purpose:** Makes representations smaller and more manageable, reducing computational cost and controlling overfitting.\n",
      "*   **Operation:** Operates *independently* on each activation map. It only downsamples *spatially*, the depth remains unchanged.\n",
      "*   **Max Pooling:** A common type where, for a given region (e.g., 2x2), the maximum value is taken as the output. This can be interpreted as detecting if a feature is present *anywhere* in that region.\n",
      "*   **No Parameters:** Pooling layers are fixed functions and do not have learnable parameters.\n",
      "*   **Common Settings:** Filter size 2x2 or 3x3, with a stride of 2.\n",
      "*   **Stride vs. Pooling:** Both downsample. Recent architectures sometimes prefer strided convolutions over explicit pooling layers.\n",
      "\n",
      "**8. Fully Connected Layer (FC Layer):**\n",
      "*   **Purpose:** Typically the final layer(s) in a CNN.\n",
      "*   **Operation:** Takes the output of the last convolutional/pooling layer (which is a 3D volume), flattens it into a 1D vector, and connects every neuron in the FC layer to every value in this flattened input.\n",
      "*   This is where the high-level features learned by the convolutional layers are aggregated to make final predictions (e.g., class scores).\n",
      "\n",
      "**9. Overall ConvNet Architecture:**\n",
      "*   A typical CNN is a sequence of **Convolutional Layers** (often followed by a non-linear activation function like ReLU), interspersed with **Pooling Layers**.\n",
      "*   This sequence is then followed by one or more **Fully Connected Layers**, and finally a **Softmax** layer for classification.\n",
      "*   **Trends:**\n",
      "    *   Towards smaller filters (e.g., 3x3) and deeper architectures.\n",
      "    *   Increasingly, architectures are moving towards getting rid of explicit pooling and fully connected layers, relying more on strided convolutions for downsampling and very deep convolutional stacks.\n",
      "    *   Typical architecture pattern: `[(CONV-RELU)*N-POOL?]*M-(FC-RELU)*K,SOFTMAX` where N is usually up to ~5, M is large, and K is 0-2.\n",
      "    *   Newer architectures like ResNet and GoogLeNet challenge this traditional paradigm.\n",
      "\n",
      "Follow-up Response:\n",
      "assistant: Slide 5 introduces the foundational concept of the **Perceptron** and its first physical implementation, the **Mark I Perceptron machine**, developed by **Frank Rosenblatt in 1957**.\n",
      "\n",
      "The main points discussed are:\n",
      "*   **First Implementation:** The Mark I Perceptron machine was the very first physical implementation of the perceptron algorithm.\n",
      "*   **Functionality:** It was designed to recognize letters of the alphabet. It processed a 400-pixel image (from a 20x20 array of cadmium sulfide photocells) and produced a binary output (either 1 or 0) based on a linear score function (f(x) = Wx + b).\n",
      "*   **Learning Mechanism:** The perceptron used an \"update rule\" to adjust its weights. The lecture highlights that, while this rule might appear similar to modern backpropagation, there was **no principled backpropagation technique** at this stage. Instead, weights were directly adjusted in the direction of the desired target.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    api_key = \"your_api_key\"  # Replace with your actual API key\n",
    "    initial_query = \"Summarize the key points from the lecture presentation and audio.\"\n",
    "    follow_up_query = \"Explain the main concept discussed in slide 5 of the presentation.\"\n",
    "    pdf_path = \"sample_lecture_slides.pdf\"  # Replace with a real PDF path\n",
    "    audio_path = \"sample_lecture_audio.mp3\"  # Replace with a real audio path\n",
    "\n",
    "    print(\"Uploading and caching files...\")\n",
    "    cache_name = upload_and_cache_files(pdf_path, audio_path, api_key)\n",
    "    llm = GoogleGenAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        api_key=api_key,\n",
    "        cached_content=cache_name\n",
    "    )\n",
    "    \n",
    "    # Initial query\n",
    "    messages = create_qa_messages(initial_query)\n",
    "    initial_response = llm.chat(messages)\n",
    "    print(\"\\nInitial Response:\")\n",
    "    print(initial_response)\n",
    "\n",
    "    # Add AI response to messages for multi-turn\n",
    "    messages.append(ChatMessage(role=\"assistant\", content=str(initial_response)))\n",
    "    \n",
    "    # Follow-up query\n",
    "    messages = create_qa_messages(follow_up_query, previous_messages=messages)\n",
    "    follow_up_response = llm.chat(messages)\n",
    "    print(\"\\nFollow-up Response:\")\n",
    "    print(follow_up_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
