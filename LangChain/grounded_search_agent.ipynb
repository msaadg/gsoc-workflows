{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc54951",
   "metadata": {},
   "source": [
    "# Advanced Grounded Search Agent for Real-Time Research Assistance\n",
    "\n",
    "## Overview and Purpose\n",
    "This comprehensive research workflow leverages the power of **two specialized AI agents** to provide thorough, fact-checked research responses. The system combines:\n",
    "\n",
    "- **Real-time Google Search grounding** for current, accurate information\n",
    "- **Multi-document PDF processing** (support for multiple files or none)\n",
    "- **Structured JSON output** with detailed research components\n",
    "- **Two-agent architecture** that separates search from synthesis for optimal performance\n",
    "\n",
    "## Architecture\n",
    "The workflow employs a **dual-agent design** to overcome API limitations:\n",
    "\n",
    "1. **Search Agent**: Uses Google Search grounding tools to gather current information from the web\n",
    "2. **Synthesis Agent**: Processes search results + PDF content to create structured, comprehensive responses\n",
    "\n",
    "This separation ensures both tools and structured output work seamlessly together.\n",
    "\n",
    "## Use Cases\n",
    "Perfect for:\n",
    "- Academic researchers needing current literature reviews\n",
    "- Business analysts requiring market research with document analysis\n",
    "- Students combining course materials with current developments\n",
    "- Developers researching latest frameworks and best practices\n",
    "\n",
    "## Import Required Libraries\n",
    "Essential dependencies for multimodal AI research agent functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.ai.generativelanguage_v1beta.types import Tool as GenAITool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4dcbd",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "Before running the agents, ensure you have:\n",
    "\n",
    "1. **Google Gemini API Key**: Get from [Google AI Studio](https://aistudio.google.com/)\n",
    "2. **PDF Files**: Place PDF files in the same directory or provide full paths\n",
    "3. **Internet Connection**: Required for Google Search grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e24e6",
   "metadata": {},
   "source": [
    "## Enhanced Structured Output Models\n",
    "Defines comprehensive Pydantic models for organizing research output:\n",
    "\n",
    "### SearchResults Model\n",
    "Structures findings from the search agent with query tracking, key findings, and source URLs.\n",
    "\n",
    "### ResearchResponse Model  \n",
    "Enhanced output structure includes:\n",
    "- **Query & Answer**: Original query with comprehensive answer breakdown\n",
    "- **Search Insights**: Key findings from web research with sources\n",
    "- **File Insights**: Analysis from uploaded PDF documents\n",
    "- **Synthesis**: Intelligent combination of all information sources\n",
    "\n",
    "This ensures responses are well-organized, traceable, and machine-readable for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4347b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced models for better structured output\n",
    "\n",
    "class SearchInsight(BaseModel):\n",
    "    \"\"\"Individual search finding with source attribution\"\"\"\n",
    "    finding: str = Field(description=\"A key finding or insight from search\")\n",
    "    source_url: str = Field(description=\"The URL where this information was found\")\n",
    "    relevance: str = Field(description=\"Brief explanation of why this is relevant\")\n",
    "\n",
    "class FileInsight(BaseModel):\n",
    "    \"\"\"Insights extracted from a single PDF file\"\"\"\n",
    "    filename: str = Field(description=\"Name of the PDF file\")\n",
    "    key_points: List[str] = Field(description=\"Main points extracted from this file\")\n",
    "    relevance_to_query: str = Field(description=\"How this file relates to the research query\")\n",
    "    summary: str = Field(description=\"Brief summary of the file's content\")\n",
    "\n",
    "class ResearchAnswer(BaseModel):\n",
    "    \"\"\"Comprehensive answer structure\"\"\"\n",
    "    summary: str = Field(description=\"Concise overview of findings\")\n",
    "    detailed_analysis: str = Field(description=\"In-depth analysis and explanation\")\n",
    "    key_findings: List[str] = Field(description=\"Bulleted list of main discoveries\")\n",
    "    conclusion: str = Field(description=\"Final synthesis and recommendations\")\n",
    "\n",
    "class ResearchResponse(BaseModel):\n",
    "    \"\"\"Complete structured research response\"\"\"\n",
    "    query: str = Field(description=\"The original research query\")\n",
    "    answer: ResearchAnswer = Field(description=\"Comprehensive answer structure\")\n",
    "    search_insights: List[SearchInsight] = Field(description=\"Detailed findings from web search\")\n",
    "    search_sources: List[str] = Field(description=\"All source URLs found during search\")\n",
    "    file_insights: List[FileInsight] = Field(description=\"Analysis from all provided PDF files\")\n",
    "    synthesis: str = Field(description=\"Integration of search results and file content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3391c9e",
   "metadata": {},
   "source": [
    "## Initialize LLM with Structured Output\n",
    "Initializes the Gemini 2.5 Flash model with streaming callbacks for real-time output and configures it to return structured responses using the `ResearchResponse` model. The `google_api_key` must be replaced with a valid API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM for search agent (with tools)\n",
    "search_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    google_api_key=\"your-api-key\"  # Replace with your actual API key\n",
    ")\n",
    "\n",
    "# LLM for synthesis agent (structured output)\n",
    "synthesis_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    google_api_key=\"your-api-key\"  # Replace with your actual API key\n",
    ")\n",
    "\n",
    "# Configure structured output for the synthesis agent only\n",
    "structured_synthesis_llm = synthesis_llm.with_structured_output(ResearchResponse, method=\"json_mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d0887",
   "metadata": {},
   "source": [
    "## PDF Processing Utilities\n",
    "Utilities for handling PDF files as base64-encoded content for multimodal processing:\n",
    "\n",
    "- **Single PDF**: Convert one PDF file to base64 for agent processing\n",
    "- **Multiple PDFs**: Process a list of PDF files for comprehensive document analysis\n",
    "- **Optional PDFs**: Support workflows with or without document inputs\n",
    "\n",
    "These functions enable flexible document processing workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bbe4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_as_base64(pdf_path: str) -> str:\n",
    "    \"\"\"Load a single PDF file and encode as base64\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_bytes = file.read()\n",
    "    return base64.b64encode(pdf_bytes).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dce8e4",
   "metadata": {},
   "source": [
    "## Search Agent Functions\n",
    "Creates functions for the first agent that performs internet search using Google Search grounding to gather information about the query. This agent focuses solely on retrieving relevant information without structured output constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe3103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_search_agent(query: str) -> str:\n",
    "    \"\"\"\n",
    "    First agent: Performs internet search using Google Search grounding\n",
    "    Returns raw search results and findings as text\n",
    "    \"\"\"\n",
    "    search_message = HumanMessage(content=f\"\"\"\n",
    "    Research the following query thoroughly using internet search:\n",
    "    {query}\n",
    "    \n",
    "    Please provide:\n",
    "    1. Key findings and recent information\n",
    "    2. Important sources and URLs\n",
    "    3. Relevant facts and data\n",
    "    4. Any recent developments or trends\n",
    "    \n",
    "    Focus on comprehensive information gathering from reliable sources.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Use tools for grounding with Google Search\n",
    "    response = search_llm.invoke(\n",
    "        [search_message],\n",
    "        tools=[GenAITool(google_search={})]\n",
    "    )\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544dc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthesis_message(query: str, search_results: str, pdf_paths: List[str] = None) -> HumanMessage:\n",
    "    \"\"\"\n",
    "    Creates a message for the synthesis agent that combines search results with PDF content\n",
    "    \"\"\"\n",
    "    content = [{\n",
    "        \"type\": \"text\", \n",
    "        \"text\": f\"\"\"\n",
    "        Based on the search results below and the provided PDF (if any), create a comprehensive structured response for this query: {query}\n",
    "\n",
    "        SEARCH RESULTS:\n",
    "        {search_results}\n",
    "\n",
    "        Please synthesize this information with any insights from the PDF to provide a complete research response.\n",
    "        Return your response in the structured format with query, summary, sources, pdf_insights, and synthesis fields.\n",
    "        \"\"\"\n",
    "    }]\n",
    "\n",
    "    if pdf_paths:\n",
    "        for pdf_path in pdf_paths:\n",
    "            pdf_base64 = load_pdf_as_base64(pdf_path)\n",
    "            content.append({\n",
    "                \"type\": \"file\",\n",
    "                \"source_type\": \"base64\",\n",
    "                \"mime_type\": \"application/pdf\",\n",
    "                \"data\": pdf_base64\n",
    "            })\n",
    "\n",
    "    return HumanMessage(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50656ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_synthesis_agent(query: str, search_results: str, pdf_path: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Second agent: Synthesizes search results with PDF content using structured output\n",
    "    Returns structured ResearchResponse\n",
    "    \"\"\"\n",
    "    synthesis_message = create_synthesis_message(query, search_results, pdf_path)\n",
    "    \n",
    "    # Use structured output (no tools needed)\n",
    "    response = structured_synthesis_llm.invoke([synthesis_message])\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d34d4",
   "metadata": {},
   "source": [
    "## Main Research Agent Orchestrator\n",
    "Orchestrates the two-agent workflow: first calls the search agent to gather information from the internet using Google Search grounding, then calls the synthesis agent to combine search results with PDF insights and produce structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d98c7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_research_agent(query: str, pdf_path: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Main orchestrator function that coordinates the two-agent workflow\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Running search agent to gather information from the internet...\")\n",
    "    search_results = run_search_agent(query)\n",
    "    \n",
    "    print(\"\\nStep 2: Running synthesis agent to combine search results with PDF content...\")\n",
    "    final_response = run_synthesis_agent(query, search_results, pdf_path)\n",
    "    \n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694020d",
   "metadata": {},
   "source": [
    "## Execute Two-Agent Workflow\n",
    "Runs the complete two-agent workflow: first agent searches the internet for information using Google Search grounding, then the second agent synthesizes the search results with PDF content to produce a structured response. This approach separates tool usage from structured output to avoid compatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f941e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Running search agent to gather information from the internet...\n",
      "\n",
      "Step 2: Running synthesis agent to combine search results with PDF content...\n",
      "\n",
      "Final Research Response:\n",
      "query='Summarize recent advancements in model compression for LLMs from 2024 and 2025' answer=ResearchAnswer(summary='Recent advancements in Large Language Model (LLM) compression (2024-2025) emphasize efficiency and practical deployment over sheer model size. Key techniques like quantization, pruning, and knowledge distillation continue to evolve, with a strong focus on post-training methods and hardware-aware optimization. A significant development is the rise of Mixture-of-Experts (MoE) architectures, which enable massive models with efficient inference by activating only a subset of parameters. Knowledge distillation is being leveraged to transfer complex reasoning and emergent abilities to smaller models. These efforts aim to reduce the high computational costs and memory requirements of LLMs, making them more accessible, faster, and more cost-effective for real-world applications. Open-source models and multi-model deployment strategies are further democratizing LLM utilization.', detailed_analysis=\"Recent advancements in Large Language Model (LLM) compression during 2024 and 2025 reflect a strategic shift from merely increasing model size to prioritizing efficiency, specialized capabilities, and sustainable deployment. The core objective is to make LLMs more accessible and cost-effective, particularly for inference.\\n\\n**1. Dominant Compression Techniques and Their Evolution:**\\n    *   **Quantization:** This remains a primary method, reducing the precision of model parameters. Advances include: \\n        *   **Post-Training Quantization (PTQ):** Increasingly favored due to high retraining costs, PTQ efficiently converts full-precision models to low-precision (e.g., 4-bit, 8-bit, or even 1-bit as seen with OneBit and QuZO framework in Feb 2025) without retraining. Specific techniques like Weight-Only (e.g., LUT-GEMM, GPTQ, AWQ, SqueezeLLM, OWQ, SpQR), Weight-Activation (e.g., ZeroQuant, SmoothQuant, OmniQuant, SplitQuantV2), and KV Cache Quantization (e.g., KVQuant, KIVI, WKVQuant) are being refined. SplitQuantV2 (Mar 2025) showed significant accuracy improvements for INT4 quantization on Llama 3.2. QuZO (Feb 2025) reduced memory costs for fine-tuning with low-precision forward passes. Hardware-aware optimization for specific bit-widths is crucial.\\n        *   **Quantization-Aware Training (QAT):** While requiring retraining, QAT (e.g., LLM-QAT, BitDistiller, L4Q) is being explored with Parameter-Efficient Fine-Tuning (PEFT) methods like QLORA, PEQA, and LoftQ to mitigate retraining costs.\\n    *   **Pruning:** This involves removing redundant parameters to create sparser models. \\n        *   **Unstructured Pruning:** Methods like SparseGPT and Wanda (2024) achieve significant sparsity (e.g., 50%) with minimal performance loss, often without retraining. Flash-LLM (2023) supports hardware acceleration for unstructured sparsity.\\n        *   **Structured Pruning:** This removes entire components (e.g., layers, attention heads) and is hardware-agnostic, though it may require fine-tuning for performance recovery. Shortened LLaMA (2024) focuses on depth pruning by removing unimportant Transformer blocks. A simple approach of pruning the final 25% of layers in Llama-3.1-8B-It showed strong performance.\\n        *   **Semi-Structured Pruning:** N:M sparsity (e.g., 2:4 sparsity in Sparse-Llama-3.1-8B-2of4, which achieved 98% recovery and up to 30% higher throughput) is gaining traction for its hardware compatibility (e.g., NVIDIA Ampere Tensor Cores) and efficiency gains. Bonsai (Apr 2025) offers gradient-free structured pruning.\\n    *   **Knowledge Distillation (KD):** This transfers knowledge from a large 'teacher' model to a smaller 'student' model. \\n        *   **Evolution of KD:** Beyond simple compression, KD is increasingly used to transfer complex AI capabilities like reasoning patterns and alignment strategies. Multi-teacher distillation is emerging. Self-Evolution Knowledge Distillation (Dec 2024) adapts prior knowledge based on learning difficulty. \\n        *   **Black-box KD:** Leverages outputs from closed-source LLMs (e.g., ChatGPT, GPT-4) to train smaller models. Focus areas include distilling emergent abilities such as Chain-of-Thought (CoT) (e.g., MT-COT, Self-Evolution KD, PaD for Program-of-Thought), In-Context Learning (e.g., ICL Distillation, AICD), and Instruction Following (e.g., Lion, LaMini-LM, SELF-INSTRUCT).\\n        *   **White-box KD:** Utilizes internal structures and knowledge representations of open-source teacher models (e.g., MINILLM, GKD, TED) for deeper knowledge transfer.\\n    *   **Low-Rank Factorization:** Decomposes large matrices into smaller ones to save space and computation (e.g., LPLR, ASVD, LASER).\\n\\n**2. Emerging Architectures and Strategies:**\\n    *   **Mixture-of-Experts (MoE) Architectures:** A significant advancement, MoE models activate only a subset of parameters for each input, drastically reducing computational demands during inference. This allows for models with trillions of parameters while keeping active parameter counts small (e.g., Mixtral-8x22B uses 39B active out of 141B total). This is a key driver for efficiency.\\n    *   **Hardware-Aware Compression:** Optimizing compressed models for specific hardware (GPUs, TPUs, NPUs, edge AI devices) is crucial for maximizing real-world efficiency and deployment in resource-constrained environments.\\n    *   **Post-Training Methods Focus:** Given the high cost of retraining, research heavily emphasizes post-training pruning and quantization to achieve significant compression without intensive retraining.\\n    *   **Dynamic and Adaptive Compression:** New methods dynamically adjust compression based on input or task requirements, leading to better performance and efficiency.\\n    *   **Multimodal LLMs and Compression:** Compression techniques are adapting to efficiently handle diverse data types (text, images, audio, video) as multimodal LLMs (e.g., Grok-1.5V, Gemini 1.5) become more prevalent.\\n    *   **Democratization through Open Source:** Open-source LLMs (LLaMA 3.1, Mixtral, Gemma 2) and improved open-source inference frameworks are lowering barriers to entry, making LLM deployment more accessible.\\n    *   **Multi-Model Strategies:** Enterprises are increasingly deploying multiple specialized compressed models and intelligently routing workloads to reduce overall inference costs (reported 40-60% reduction).\\n\\n**3. Performance and Market Impact:**\\n    *   GPT-175B requires significant memory (350GB FP16) and GPUs (5x A100). Model compression directly addresses these challenges.\\n    *   Inference costs account for 70-85% of ongoing LLM deployment expenses, highlighting the economic importance of compression.\\n    *   Smaller, compressed models are achieving performance comparable to much larger counterparts (e.g., Gemma 2 (27B) performs like models twice its size; DeepSeek R1 (37B active) comparable to OpenAI's o1 at lower cost).\\n    *   Specific examples like Sparse-Llama-3.1-8B-2of4 showing 30% higher throughput and 20% lower latency, and SplitQuantV2 improving INT4 quantization accuracy significantly, demonstrate tangible gains.\\n\\n**4. Persistent Challenges:** Data privacy, integration complexity, and maintaining performance at scale while optimizing resource allocation remain key challenges in LLM deployment, even with advancements in compression.\", key_findings=['The paradigm is shifting from maximizing LLM size to prioritizing efficiency, specialized capabilities, and sustainable deployment, especially for inference.', 'Quantization (PTQ, QAT, KV Cache), pruning (unstructured, structured, semi-structured like N:M sparsity), and knowledge distillation are the foundational compression techniques, with continuous advancements.', 'Mixture-of-Experts (MoE) architectures are a significant advancement, enabling models with trillions of parameters while only activating a small subset for inference, dramatically improving efficiency.', 'Hardware-aware compression is crucial, optimizing models for specific hardware (GPUs, TPUs, NPUs, edge devices) to maximize real-world deployment efficiency.', \"There's a strong trend towards post-training compression methods due to the prohibitive costs of retraining massive LLMs.\", 'Knowledge Distillation is evolving to transfer complex emergent abilities (e.g., Chain-of-Thought, In-Context Learning, Instruction Following) from large teachers to smaller student models.', 'Smaller, compressed LLMs like Gemma 2 and DeepSeek R1 are achieving performance comparable to much larger models, demonstrating the effectiveness of compression.', 'Open-source LLMs and improved tooling are democratizing LLM deployment by lowering the barrier to entry.', 'Enterprises are adopting multi-model strategies, intelligently routing workloads to specialized compressed models, leading to significant cost reductions in inference.'], conclusion='The landscape of LLM compression is rapidly evolving, driven by the imperative to make powerful models more accessible, cost-effective, and sustainable for real-world deployment. While traditional methods like quantization, pruning, and knowledge distillation remain foundational, recent advancements in 2024 and 2025 emphasize practical considerations such as inference efficiency, hardware-aware optimization, and the emergence of sparse architectures like Mixture-of-Experts. The trend towards post-training compression and the focus on distilling emergent abilities into smaller models are key to overcoming the immense computational costs of large LLMs. Future research will likely continue to explore dynamic and adaptive compression, integrate AutoML for design, and address challenges related to explainability, data privacy, and seamless integration of compressed models into diverse hardware environments, ultimately fostering a more democratized and efficient AI ecosystem.') search_insights=[SearchInsight(finding='The focus in LLM development has shifted from simply increasing model size to prioritizing efficiency, specialized capabilities, and sustainable deployment, particularly for inference.', source_url='https://vamsitalkstech.com/the-evolution-of-large-language-models-in-2024-and-where-we-are-headed-in-2025-a-technical-review/', relevance='This highlights the overarching trend driving model compression in 2024-2025, emphasizing the practical need for efficiency.'), SearchInsight(finding='Mixture-of-Experts (MoE) architectures have emerged as a significant advancement, activating only a subset of parameters for each input, dramatically reducing computational demands during training and inference.', source_url='https://chroniclejournal.com/the-sparse-revolution-mixture-of-experts-architectures-propel-llms-into-a-new-era-of-efficiency-and-scale/', relevance='MoE is presented as a key architectural innovation in 2024-2025 directly leading to inference efficiency and enabling very large models with lower active parameter counts.'), SearchInsight(finding='There is a growing emphasis on hardware-aware compression, optimizing models for specific hardware like GPUs, TPUs, NPUs, and edge AI devices to maximize efficiency.', source_url='https://www.labelyourdata.com/blog/llm-inference-techniques-for-optimized-deployment-in-2025/', relevance='This points to a practical and critical trend in 2024-2025, ensuring compressed models are effectively deployed in diverse real-world scenarios.'), SearchInsight(finding='Due to the high cost of retraining LLMs, significant research is focused on post-training pruning and quantization methods that reduce model size and accelerate inference without requiring extensive retraining.', source_url='https://arxiv.org/abs/2402.10631', relevance='This highlights a major practical driver for compression research in 2024-2025, focusing on methods that are less resource-intensive to implement.'), SearchInsight(finding='Knowledge Distillation is evolving beyond simple compression to transfer advanced AI capabilities, including reasoning patterns and alignment strategies, from larger to smaller models, with multi-teacher distillation emerging.', source_url='https://medium.com/@sarvesh_sahu/knowledge-distillation-in-the-era-of-large-language-models-llms-f81d5b32187f', relevance='This shows a qualitative shift in KD applications in 2024-2025, emphasizing the transfer of complex intelligence rather than just size reduction.'), SearchInsight(finding='Sparse-Llama-3.1-8B-2of4, a 50% pruned version, achieved 98% recovery on Open LLM Leaderboard v1 and delivered up to 30% higher throughput and 20% lower latency with vLLM on NVIDIA Ampere GPUs (Feb 2025).', source_url='https://redhatdeveloper.com/articles/2-4-sparse-llama-smaller-models-efficient-gpu-inference', relevance='This provides a concrete example of a recent (2025) successful pruning technique with quantifiable performance gains, demonstrating the effectiveness of semi-structured sparsity.'), SearchInsight(finding='\"SplitQuantV2\" improved INT4 quantization accuracy for Llama 3.2 1B Instruct by 11.76%p on ARC dataset, matching original floating-point accuracy in 2 minutes 6 seconds on an Apple M4 CPU (Mar 2025).', source_url='https://arxiv.org/abs/2503.03714', relevance='This is a very specific, recent (2025) example of advanced quantization achieving high accuracy with minimal computational cost, showcasing efficiency gains.'), SearchInsight(finding='The \"Quantized Zeroth-Order (QuZO)\" framework for fine-tuning LLMs with low-precision (4- or 8-bit) forward passes reduced memory cost by 2.94x in LLaMA2-7B fine-tuning (Feb 2025).', source_url='https://arxiv.org/abs/2502.08845', relevance='Another specific, recent (2025) example highlighting memory efficiency gains in fine-tuning through advanced quantization.'), SearchInsight(finding='Enterprises are adopting multi-model strategies, deploying multiple specialized models instead of single general-purpose systems, and intelligently routing workloads to reduce overall inference costs by 40% to 60%.', source_url='https://www.labelyourdata.com/blog/llm-inference-techniques-for-optimized-deployment-in-2025/', relevance='This reveals a significant strategic trend in LLM deployment in 2024-2025, where compression facilitates the use of a portfolio of models for cost optimization.'), SearchInsight(finding='Dynamic and adaptive compression methods are emerging, adjusting compression based on factors like token learning difficulty (\"Self-Evolution KD\" Dec 2024) or using gradient-free pruning (\"Bonsai\" Apr 2025).', source_url='https://arxiv.org/abs/2412.12879', relevance='This points to a forward-looking trend in 2024-2025 towards more intelligent and flexible compression strategies.')] search_sources=['https://vamsitalkstech.com/the-evolution-of-large-language-models-in-2024-and-where-we-are-headed-in-2025-a-technical-review/', 'http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00704/2482209/tacl_a_00704.pdf', 'https://arxiv.org/abs/2404.13529', 'https://chroniclejournal.com/the-sparse-revolution-mixture-of-experts-architectures-propel-llms-into-a-new-era-of-efficiency-and-scale/', 'https://www.labelyourdata.com/blog/llm-inference-techniques-for-optimized-deployment-in-2025/', 'https://arxiv.org/abs/2412.12879', 'https://medium.com/@sarvesh_sahu/knowledge-distillation-in-the-era-of-large-language-models-llms-f81d5b32187f', 'https://openreview.net/forum?id=V2c24l2GzF', 'https://arxiv.org/abs/2501.10977', 'https://arxiv.org/abs/2502.09915', 'https://arxiv.org/abs/2503.03714', 'https://arxiv.org/abs/2502.08845', 'https://redhatdeveloper.com/articles/2-4-sparse-llama-smaller-models-efficient-gpu-inference', 'https://sparsity.ai/sllm-iclr-2025', 'https://www.thestateofllminference.com/', 'https://www.netapp.com/blog/top-open-source-llms/', 'https://github.com/cedrickchee/awesome-llm-compression', 'https://www.youtube.com/watch?v=1jIq06p7h9E'] file_insights=[FileInsight(filename='A Survey on Model Compression for Large Language Models.pdf', key_points=['LLMs, despite their strong performance, pose significant challenges due to their large size and computational demands (e.g., GPT-175B requires 350GB memory and 5 A100 GPUs for inference).', 'Model compression transforms large, resource-intensive models into compact versions suitable for resource-constrained deployment, enhancing inference speed and resource efficiency.', 'The survey categorizes model compression methods for LLMs into Quantization, Pruning, Knowledge Distillation, and Low-Rank Factorization.', 'Quantization reduces parameter precision (e.g., from FP16 to INT8/INT4/INT1) and is divided into Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). Sub-categories include Weight-Only, Weight-Activation, and KV Cache Quantization.', 'Pruning reduces model size by removing redundant components, categorized as Unstructured, Structured, and Semi-Structured (e.g., N:M sparsity). Structured pruning is hardware-agnostic but may require fine-tuning for recovery.', \"Knowledge Distillation transfers knowledge from a large 'teacher' model to a smaller 'student' model, with approaches like Black-box KD (using teacher outputs) and White-box KD (using teacher internal structures). It's used to distill emergent abilities like Chain-of-Thought, In-Context Learning, and Instruction Following.\", 'Low-Rank Factorization reduces large matrices into smaller ones (e.g., W ≈ UV) to save space and computation.', 'The paper highlights challenges and future directions, including the need for more advanced methods, scaling up existing compression techniques, improving LLM inference and deployment efficiency on specific hardware, understanding the effect of scaling laws, integrating AutoML for compression, and enhancing explainability.'], relevance_to_query=\"This PDF is highly relevant as it is a survey specifically on LLM compression, published in November 2024, directly aligning with the 'recent advancements' and '2024' timeframe of the query. It provides a foundational taxonomy and detailed explanations of the core compression techniques (quantization, pruning, knowledge distillation, low-rank factorization) and their sub-types, along with evaluation metrics and future challenges. This forms a strong technical backbone for the response, complementing the more current trends and specific model examples from the search results.\", summary='This 2024 survey paper provides a comprehensive overview of model compression techniques for Large Language Models (LLMs). It details methods such as quantization, pruning, knowledge distillation, and low-rank factorization, explaining their mechanisms, sub-categories, and typical applications. The paper also discusses key metrics for evaluating compressed LLMs and outlines prevailing challenges and future research directions in making LLMs more efficient and deployable in resource-constrained environments.')] synthesis=\"Recent advancements in LLM compression during 2024 and 2025 underscore a critical shift towards practical deployment and efficiency, moving beyond simply scaling up model size. Both the search results and the provided PDF, 'A Survey on Model Compression for Large Language Models' (published Nov 2024), highlight the foundational role of quantization, pruning, and knowledge distillation, while also revealing newer trends and practical considerations.\\n\\nThe PDF provides a comprehensive academic taxonomy of these core techniques: Quantization (QAT, PTQ, with sub-types like Weight-Only, Weight-Activation, and KV Cache), Pruning (Unstructured, Structured, Semi-Structured), and Knowledge Distillation (Black-box, White-box, including distillation of emergent abilities like Chain-of-Thought). It details the mechanisms and challenges, such as the computational demands of GPT-175B and the need for high-quality calibration data.\\n\\nComplementing this foundational understanding, the search results illuminate the cutting-edge developments in 2024-2025. They emphasize the rising importance of Mixture-of-Experts (MoE) architectures, which were not explicitly categorized as a compression technique in the PDF's taxonomy but are a key sparsity-driven efficiency innovation. The search results also highlight the strong focus on post-training compression methods due to high retraining costs, hardware-aware optimization for diverse devices, and the evolution of knowledge distillation to transfer complex reasoning skills rather than just model size. Specific 2025 examples like Sparse-Llama-3.1-8B-2of4, SplitQuantV2, and QuZO demonstrate tangible performance gains and efficiency improvements in real-world scenarios. The search results also introduce broader strategic trends, such as the democratization of LLMs through open-source initiatives and the adoption of multi-model strategies by enterprises to optimize inference costs.\\n\\nIn essence, the PDF provides the 'what' and 'how' of established compression techniques as of late 2024, detailing their technical underpinnings. The search results then provide the 'what's new' and 'why it matters' for 2024-2025, focusing on practical implications, emerging architectures, and the economic drivers behind the push for more efficient LLMs. Together, they paint a comprehensive picture of a field rapidly innovating to make large language models more accessible and sustainable for widespread use.\"\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = \"Summarize recent advancements in model compression for LLMs from 2024 and 2025\"\n",
    "    pdf_paths = [\"sample.pdf\"]  # Replace with paths to sample PDFs\n",
    "    result = run_research_agent(query, pdf_paths)\n",
    "    print(\"\\nFinal Research Response:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de4b9a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESEARCH SUMMARY\n",
      "==================================================\n",
      "Query: Summarize recent advancements in model compression for LLMs from 2024 and 2025\n",
      "Summary: Recent advancements in Large Language Model (LLM) compression (2024-2025) emphasize efficiency and practical deployment over sheer model size. Key techniques like quantization, pruning, and knowledge distillation continue to evolve, with a strong focus on post-training methods and hardware-aware optimization. A significant development is the rise of Mixture-of-Experts (MoE) architectures, which enable massive models with efficient inference by activating only a subset of parameters. Knowledge distillation is being leveraged to transfer complex reasoning and emergent abilities to smaller models. These efforts aim to reduce the high computational costs and memory requirements of LLMs, making them more accessible, faster, and more cost-effective for real-world applications. Open-source models and multi-model deployment strategies are further democratizing LLM utilization.\n",
      "\n",
      "Key Findings (9):\n",
      "   1. The paradigm is shifting from maximizing LLM size to prioritizing efficiency, specialized capabilities, and sustainable deployment, especially for inference.\n",
      "   2. Quantization (PTQ, QAT, KV Cache), pruning (unstructured, structured, semi-structured like N:M sparsity), and knowledge distillation are the foundational compression techniques, with continuous advancements.\n",
      "   3. Mixture-of-Experts (MoE) architectures are a significant advancement, enabling models with trillions of parameters while only activating a small subset for inference, dramatically improving efficiency.\n",
      "   4. Hardware-aware compression is crucial, optimizing models for specific hardware (GPUs, TPUs, NPUs, edge devices) to maximize real-world deployment efficiency.\n",
      "   5. There's a strong trend towards post-training compression methods due to the prohibitive costs of retraining massive LLMs.\n",
      "   6. Knowledge Distillation is evolving to transfer complex emergent abilities (e.g., Chain-of-Thought, In-Context Learning, Instruction Following) from large teachers to smaller student models.\n",
      "   7. Smaller, compressed LLMs like Gemma 2 and DeepSeek R1 are achieving performance comparable to much larger models, demonstrating the effectiveness of compression.\n",
      "   8. Open-source LLMs and improved tooling are democratizing LLM deployment by lowering the barrier to entry.\n",
      "   9. Enterprises are adopting multi-model strategies, intelligently routing workloads to specialized compressed models, leading to significant cost reductions in inference.\n",
      "\n",
      "Search Sources (18):\n",
      "   1. https://vamsitalkstech.com/the-evolution-of-large-language-models-in-2024-and-where-we-are-headed-in-2025-a-technical-review/\n",
      "   2. http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00704/2482209/tacl_a_00704.pdf\n",
      "   3. https://arxiv.org/abs/2404.13529\n",
      "   ... and 15 more sources\n",
      "\n",
      "Files Analyzed (1):\n",
      "   A Survey on Model Compression for Large Language Models.pdf: This 2024 survey paper provides a comprehensive overview of model compression techniques for Large Language Models (LLMs). It details methods such as quantization, pruning, knowledge distillation, and low-rank factorization, explaining their mechanisms, sub-categories, and typical applications. The ...\n",
      "\n",
      "Synthesis: Recent advancements in LLM compression during 2024 and 2025 underscore a critical shift towards practical deployment and efficiency, moving beyond simply scaling up model size. Both the search results and the provided PDF, 'A Survey on Model Compression for Large Language Models' (published Nov 2024), highlight the foundational role of quantization, pruning, and knowledge distillation, while also revealing newer trends and practical considerations.\n",
      "\n",
      "The PDF provides a comprehensive academic taxo...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Utility functions for enhanced workflow\n",
    "\n",
    "def display_research_summary(result: ResearchResponse) -> None:\n",
    "    \"\"\"Pretty print research results summary\"\"\"\n",
    "    print(\"RESEARCH SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Query: {result.query}\")\n",
    "    print(f\"Summary: {result.answer.summary}\")\n",
    "    print(f\"\\nKey Findings ({len(result.answer.key_findings)}):\")\n",
    "    for i, finding in enumerate(result.answer.key_findings, 1):\n",
    "        print(f\"   {i}. {finding}\")\n",
    "    \n",
    "    print(f\"\\nSearch Sources ({len(result.search_sources)}):\")\n",
    "    for i, source in enumerate(result.search_sources[:3], 1):  # Show first 3\n",
    "        print(f\"   {i}. {source}\")\n",
    "    if len(result.search_sources) > 3:\n",
    "        print(f\"   ... and {len(result.search_sources) - 3} more sources\")\n",
    "    \n",
    "    print(f\"\\nFiles Analyzed ({len(result.file_insights)}):\")\n",
    "    for insight in result.file_insights:\n",
    "        print(f\"   {insight.filename}: {insight.summary[:300]}...\")\n",
    "    \n",
    "    print(f\"\\nSynthesis: {result.synthesis[:500]}...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "display_research_summary(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
